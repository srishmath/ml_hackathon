import cv2
import numpy as np

# Preprocess face image for model input
def preprocess_face_image(face_image):
    face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
    face_image = cv2.resize(face_image, (48, 48))  # Resize to 48x48
    face_image = face_image / 255.0  # Normalize to [0, 1]
    return face_image.astype(np.float32).reshape(1, 48, 48)  # Add channel dimension for grayscale

# Function to verify face by detecting eyes and nose within the region
def is_face_with_features(face_region, eye_cascade, nose_cascade):
    eyes = eye_cascade.detectMultiScale(face_region, scaleFactor=1.1, minNeighbors=5, minSize=(15, 15))
    nose = nose_cascade.detectMultiScale(face_region, scaleFactor=1.1, minNeighbors=5, minSize=(20, 20))
    return len(eyes) >= 1 and len(nose) >= 1

# Function to extract verified faces from video using additional eye and nose detection
def extract_faces_from_video(video_path, max_faces=5, frame_skip=5):
    face_images = []
    video = cv2.VideoCapture(video_path)  # Open video file

    # Load cascades once
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
    nose_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_mcs_nose.xml')

    frame_count = 0  # Track frame count for skipping
    detected_faces = set()  # Track detected faces' positions to avoid duplicates

    while len(face_images) < max_faces:
        ret, frame = video.read()  # Read frame
        if not ret:
            break

        # Skip frames to reduce redundant detections
        if frame_count % frame_skip == 0:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
            
            for (x, y, w, h) in faces:
                face_region = gray[y:y+h, x:x+w]
                
                # Verify if detected region contains eyes and nose
                if is_face_with_features(face_region, eye_cascade, nose_cascade):
                    face_id = (x, y, w, h)
                    
                    # Ensure no duplicate faces by tracking unique face positions
                    if face_id not in detected_faces:
                        detected_faces.add(face_id)  # Mark this face as detected
                        preprocessed_face = preprocess_face_image(frame[y:y+h, x:x+w])  # Preprocess for model
                        face_images.append(preprocessed_face)
                        
                        # Break loop if maximum faces are reached
                        if len(face_images) >= max_faces:
                            break

        frame_count += 1  # Increment frame counter
    
    video.release()  # Release video capture
    return face_images

# Loading video paths from train_df
# Assuming train_df has a column 'video_clip_path' containing paths to video clips
train_videos = train_df['video_clip_path'].tolist()  # Load all video paths from the DataFrame

# Select a subset of videos - you can adjust this number as needed
num_subset = 5  # Change this number to the subset size you want
subset_videos = train_videos[:num_subset]  # Selecting the first num_subset videos

# Process each video and store the results
all_faces = {}

for video_path in subset_videos:
    faces = extract_faces_from_video(video_path)
    if faces:
        all_faces[video_path] = faces
        print(f"Faces detected in {video_path}: {len(faces)} faces.")
    else:
        all_faces[video_path] = []
        print(f"No faces detected in {video_path}.")

print("Processing complete.")
